#!/usr/bin/env python3
"""
SmartDownloaderä¸»ç¨‹åº
æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸AIå­—å¹•å·¥å…·çš„ä¸»å…¥å£ç‚¹
"""

import argparse
import asyncio
import logging
from pathlib import Path
from typing import Optional, List

from rich.console import Console

from config_manager import config_manager, config
from downloader import Downloader
from subtitles import SubtitleProcessor
from utils import setup_logging, get_inputs, sanitize
from handlers import process_local_file, process_metadata_phase, process_download_phase


console = Console()
log = logging.getLogger(__name__)


def get_cookies_configuration() -> tuple[str, str, str, bool, bool]:
    """è·å–cookiesé…ç½®ä¿¡æ¯ã€‚
    
    Returns:
        tuple: (cookies_mode, browser_type, manual_cookies_file, auto_extract_enabled, force_refresh)
    """
    cookies_config = config.cookies
    return (
        cookies_config.mode,
        cookies_config.browser_type,
        cookies_config.manual_cookies_file,
        cookies_config.auto_extract_enabled,
        cookies_config.force_refresh
    )


def handle_manual_cookies(manual_cookies_file: str) -> Optional[str]:
    """å¤„ç†æ‰‹åŠ¨cookiesæ–‡ä»¶ã€‚
    
    Args:
        manual_cookies_file (str): æ‰‹åŠ¨cookiesæ–‡ä»¶è·¯å¾„ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ‰¾åˆ°åˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    manual_cookies_path = Path(manual_cookies_file)
    if manual_cookies_path.exists():
        cookies = str(manual_cookies_path.resolve())
        console.print(f'ğŸª ä½¿ç”¨æ‰‹åŠ¨cookiesæ–‡ä»¶: {cookies}', style='green')
        return cookies
    else:
        console.print(f'âš ï¸ æœªæ‰¾åˆ°æ‰‹åŠ¨cookiesæ–‡ä»¶: {manual_cookies_file}', style='yellow')
        return None


def try_auto_extract_cookies(first_url: str, browser_type: str, cookies_config) -> Optional[str]:
    """å°è¯•è‡ªåŠ¨æå–cookiesã€‚
    
    Args:
        first_url (str): ç¬¬ä¸€ä¸ªURLã€‚
        browser_type (str): æµè§ˆå™¨ç±»å‹ã€‚
        cookies_config: cookiesé…ç½®å¯¹è±¡ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæˆåŠŸåˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    try:
        from auto_cookies import auto_extract_cookies_for_url
        
        console.print(f'ğŸª æ­£åœ¨ä¸º {first_url} ä»{browser_type}æµè§ˆå™¨è‡ªåŠ¨æå–cookies...', style='cyan')
        auto_cookies_file = auto_extract_cookies_for_url(
            first_url,
            browser_type,
            cache_enabled=cookies_config.cache_enabled,
            cache_file=cookies_config.cache_file,
            cache_duration_hours=cookies_config.cache_duration_hours
        )
        
        if auto_cookies_file and Path(auto_cookies_file).exists():
            cookies = str(Path(auto_cookies_file).resolve())
            console.print(f'âœ… æˆåŠŸè‡ªåŠ¨è·å–cookies: {cookies}', style='bold green')
            return cookies
        else:
            console.print(f'âš ï¸ æ— æ³•è‡ªåŠ¨è·å–cookiesï¼Œå°†åœ¨æ— cookiesæƒ…å†µä¸‹ç»§ç»­', style='yellow')
            return None
    except ImportError as e:
        console.print(f'âš ï¸ è‡ªåŠ¨cookiesæ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ‰‹åŠ¨æ”¾ç½®cookies.txtæ–‡ä»¶: {e}', style='yellow')
        return None
    except Exception as e:
        console.print(f'âš ï¸ è‡ªåŠ¨è·å–cookiesæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}', style='yellow')
        return None


def handle_browser_mode_cookies(inputs: List[str], browser_type: str, cookies_config, force_refresh: bool) -> Optional[str]:
    """å¤„ç†æµè§ˆå™¨æ¨¡å¼cookiesã€‚
    
    Args:
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        browser_type (str): æµè§ˆå™¨ç±»å‹ã€‚
        cookies_config: cookiesé…ç½®å¯¹è±¡ã€‚
        force_refresh (bool): æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæˆåŠŸåˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    if not cookies_config.auto_extract_enabled:
        console.print(f'âš ï¸ è‡ªåŠ¨cookiesæå–å·²ç¦ç”¨', style='yellow')
        return None
    
    if cookies_config.mode == 'browser':
        console.print(f'ğŸ” é…ç½®è®¾ç½®å¼ºåˆ¶ä»æµè§ˆå™¨è·å–cookies...', style='cyan')
    elif force_refresh:
        console.print(f'ğŸ”„ é…ç½®è®¾ç½®å¼ºåˆ¶åˆ·æ–°cookies...', style='cyan')
    
    if inputs:
        first_url = inputs[0]
        return try_auto_extract_cookies(first_url, browser_type, cookies_config)
    
    return None


def handle_cache_cookies(cookies_config, inputs: List[str], browser_type: str) -> Optional[str]:
    """å¤„ç†ç¼“å­˜cookiesã€‚
    
    Args:
        cookies_config: cookiesé…ç½®å¯¹è±¡ã€‚
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        browser_type (str): æµè§ˆå™¨ç±»å‹ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæˆåŠŸåˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    cache_cookies_path = Path(cookies_config.cache_file)
    
    if not (cookies_config.cache_enabled and cache_cookies_path.exists()):
        return None
    
    try:
        from auto_cookies import BrowserCookiesExtractor
        extractor = BrowserCookiesExtractor(
            cache_enabled=cookies_config.cache_enabled,
            cache_file=cookies_config.cache_file,
            cache_duration_hours=cookies_config.cache_duration_hours
        )
        
        if extractor._is_cache_valid():
            cookies = str(cache_cookies_path.resolve())
            console.print(f'ğŸª ä½¿ç”¨æœ‰æ•ˆçš„cookiesç¼“å­˜: {cookies}', style='green')
            return cookies
        else:
            console.print(f'âš ï¸ cookiesç¼“å­˜å·²è¿‡æœŸï¼Œå°è¯•è‡ªåŠ¨è·å–æ–°cookies...', style='yellow')
            if cookies_config.auto_extract_enabled and inputs:
                first_url = inputs[0]
                return try_auto_extract_cookies(first_url, browser_type, cookies_config)
            return None
    except ImportError as e:
        console.print(f'âš ï¸ è‡ªåŠ¨cookiesæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç°æœ‰ç¼“å­˜æ–‡ä»¶: {e}', style='yellow')
        return str(cache_cookies_path.resolve())
    except Exception as e:
        console.print(f'âš ï¸ æ£€æŸ¥cookiesç¼“å­˜æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}', style='yellow')
        return None


def handle_auto_mode_cookies(inputs: List[str], browser_type: str, cookies_config) -> Optional[str]:
    """å¤„ç†è‡ªåŠ¨æ¨¡å¼cookiesã€‚
    
    Args:
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        browser_type (str): æµè§ˆå™¨ç±»å‹ã€‚
        cookies_config: cookiesé…ç½®å¯¹è±¡ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæˆåŠŸåˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    manual_cookies_path = Path(cookies_config.manual_cookies_file)
    
    # ä¼˜å…ˆçº§ï¼šæ‰‹åŠ¨cookies > ç¼“å­˜cookies > è‡ªåŠ¨è·å–cookies
    if manual_cookies_path.exists():
        cookies = str(manual_cookies_path.resolve())
        console.print(f'ğŸª ä½¿ç”¨æ‰‹åŠ¨cookiesæ–‡ä»¶: {cookies}', style='green')
        return cookies
    
    # æ£€æŸ¥ç¼“å­˜cookies
    cached_cookies = handle_cache_cookies(cookies_config, inputs, browser_type)
    if cached_cookies:
        return cached_cookies
    
    # æ²¡æœ‰æ‰‹åŠ¨cookieså’Œç¼“å­˜ï¼Œå°è¯•è‡ªåŠ¨è·å–
    if cookies_config.auto_extract_enabled:
        console.print(f'ğŸ” æœªæ‰¾åˆ°æ‰‹åŠ¨cookiesæ–‡ä»¶å’Œç¼“å­˜ï¼Œå°è¯•è‡ªåŠ¨è·å–æµè§ˆå™¨cookies...', style='yellow')
        if inputs:
            first_url = inputs[0]
            return try_auto_extract_cookies(first_url, browser_type, cookies_config)
    else:
        console.print(f'âš ï¸ æœªæ‰¾åˆ°cookiesæ–‡ä»¶ä¸”è‡ªåŠ¨è·å–å·²ç¦ç”¨', style='yellow')
    
    return None


def get_cookies(inputs: List[str]) -> Optional[str]:
    """è·å–cookiesæ–‡ä»¶è·¯å¾„ã€‚
    
    Args:
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        
    Returns:
        Optional[str]: cookiesæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæˆåŠŸåˆ™è¿”å›è·¯å¾„ï¼Œå¦åˆ™è¿”å›Noneã€‚
    """
    cookies_mode, browser_type, manual_cookies_file, auto_extract_enabled, force_refresh = get_cookies_configuration()
    cookies_config = config.cookies
    
    if cookies_mode == 'skip':
        console.print(f'ğŸš« è·³è¿‡cookiesï¼ˆé…ç½®è®¾ç½®ï¼‰', style='yellow')
        return None
    
    if cookies_mode == 'manual':
        return handle_manual_cookies(manual_cookies_file)
    elif cookies_mode == 'browser' or force_refresh:
        return handle_browser_mode_cookies(inputs, browser_type, cookies_config, force_refresh)
    else:
        # autoæ¨¡å¼
        return handle_auto_mode_cookies(inputs, browser_type, cookies_config)


def process_x_com_urls(current_url_tasks: List[tuple], video_count: int, url: str) -> List[tuple]:
    """å¤„ç†X.comå¤šè§†é¢‘é“¾æ¥æƒ…å†µã€‚
    
    Args:
        current_url_tasks (List[tuple]): å½“å‰URLä»»åŠ¡åˆ—è¡¨ã€‚
        video_count (int): è§†é¢‘æ•°é‡ã€‚
        url (str): å½“å‰URLã€‚
        
    Returns:
        List[tuple]: å¤„ç†åçš„ä»»åŠ¡åˆ—è¡¨ã€‚
    """
    if video_count > 1 and ('x.com' in url or 'twitter.com' in url):
        console.print(f'âš ï¸  ä¸æ”¯æŒä¸€ä¸ªé“¾æ¥ğŸ”—é‡ŒåŒ…å«å¤šä¸ªè§†é¢‘ä¸‹è½½å“¦ï½', style='bold red')
        console.print(f'ğŸ”— å½“å‰é“¾æ¥åŒ…å« {video_count} ä¸ªè§†é¢‘ï¼Œä»…æ”¯æŒå•è§†é¢‘é“¾æ¥', style='yellow')
        console.print(f'ğŸ’¡ å»ºè®®ï¼šè¯·åˆ†åˆ«è·å–æ¯ä¸ªè§†é¢‘çš„å•ç‹¬é“¾æ¥è¿›è¡Œä¸‹è½½', style='cyan')
        console.print(f'ğŸ“¥ å°†ä»…ä¸‹è½½ç¬¬ä¸€ä¸ªè§†é¢‘...', style='bold yellow')
        
        if current_url_tasks:
            return [current_url_tasks[0]]  # åªè¿”å›ç¬¬ä¸€ä¸ªè§†é¢‘
    
    return current_url_tasks


async def collect_task_metadata(downloader: Downloader, inputs: List[str]) -> List[tuple]:
    """æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„å…ƒæ•°æ®ã€‚
    
    Args:
        downloader (Downloader): ä¸‹è½½å™¨å®ä¾‹ã€‚
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        
    Returns:
        List[tuple]: ä»»åŠ¡å…ƒæ•°æ®åˆ—è¡¨ã€‚
    """
    task_metadata = []
    i = 0
    
    for url in inputs:
        video_count = 0
        current_url_tasks = []
        
        async for meta in downloader.stream_playlist_info(url):
            video_count += 1
            i += 1
            
            # ä¸ºé¿å…å¤šè§†é¢‘åŒåå†²çªï¼Œæ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
            title = meta.get('title', f'é¡¹ç›®_{i}')
            video_id = meta.get('id', f'video_{i}')
            
            # å¦‚æœæœ‰è§†é¢‘IDï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡ä»¶åä¸­ä»¥ç¡®ä¿å”¯ä¸€æ€§
            if video_id and video_id != f'video_{i}':
                # æˆªå–è§†é¢‘IDçš„æœ€å8ä½ä½œä¸ºå”¯ä¸€æ ‡è¯†
                unique_id = str(video_id)[-8:]
                prefix = f"{i:03d}_{sanitize(title)}_{unique_id}"
            else:
                prefix = f"{i:03d}_{sanitize(title)}"
            
            current_url_tasks.append((url, prefix, meta))
        
        # å¤„ç†X.comå¤šè§†é¢‘é“¾æ¥æƒ…å†µ
        processed_tasks = process_x_com_urls(current_url_tasks, video_count, url)
        task_metadata.extend(processed_tasks)
        
        if video_count == 0:  # Handle single video URL
            i += 1
            prefix = f"001_{sanitize('å•é¡¹ä¸‹è½½')}"
            task_metadata.append((url, prefix, {'url': url}))
    
    return task_metadata


async def process_subtitle_tasks(sub_processor: SubtitleProcessor, inputs: List[str]) -> None:
    """å¤„ç†å­—å¹•ä»»åŠ¡ã€‚
    
    Args:
        sub_processor (SubtitleProcessor): å­—å¹•å¤„ç†å™¨å®ä¾‹ã€‚
        inputs (List[str]): è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
    """
    console.print(f'ğŸ§  AIå­—å¹•ç”Ÿæˆæ¨¡å¼å¯åŠ¨ï¼Œå°†å¹¶å‘å¤„ç† {len(inputs)} ä¸ªæ–‡ä»¶', style='bold cyan')
    
    tasks = []
    for file_path in inputs:
        if sub_processor is not None:
            tasks.append(process_local_file(sub_processor, file_path))
        else:
            log.error('Subtitle processor is not initialized, cannot process local file for subtitles.')
    
    if tasks:
        await asyncio.gather(*tasks)


async def process_download_tasks(downloader: Downloader, sub_processor: Optional[SubtitleProcessor], inputs: List[str], args: argparse.Namespace) -> None:
    """å¤„ç†ä¸‹è½½ä»»åŠ¡ã€‚
    
    Args:
        downloader (Downloader): ä¸‹è½½å™¨å®ä¾‹ã€‚
        sub_processor (Optional[SubtitleProcessor]): å­—å¹•å¤„ç†å™¨å®ä¾‹ã€‚
        inputs (List[str]): è¾“å…¥URLåˆ—è¡¨ã€‚
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°ã€‚
    """
    console.print(f'ğŸš€ ä¸‹è½½æ¨¡å¼å¯åŠ¨ï¼Œå°†å¹¶å‘å¤„ç† {len(inputs)} ä¸ªURL/æ’­æ”¾åˆ—è¡¨', style='bold cyan')
    
    # æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„å…ƒæ•°æ®
    task_metadata = await collect_task_metadata(downloader, inputs)
    
    # é˜¶æ®µ1ï¼šå¹¶å‘å¤„ç†æ‰€æœ‰å…ƒæ•°æ®
    metadata_tasks = []
    for url, prefix, meta in task_metadata:
        metadata_tasks.append(process_metadata_phase(downloader, meta.get('url', url), prefix))
    
    await asyncio.gather(*metadata_tasks)
    
    # é˜¶æ®µ2ï¼šé¡ºåºå¤„ç†æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
    for url, prefix, meta in task_metadata:
        await process_download_phase(downloader, sub_processor, meta.get('url', url), prefix, args)


async def main() -> None:
    """SmartDownloaderä¸»ç¨‹åºå…¥å£ç‚¹ã€‚
    
    å¤„ç†å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆå§‹åŒ–ä¸‹è½½å™¨å’Œå­—å¹•å¤„ç†å™¨ï¼Œ
    æ ¹æ®æŒ‡å®šæ¨¡å¼æ‰§è¡Œä¸‹è½½æˆ–å­—å¹•ç”Ÿæˆä»»åŠ¡ã€‚
    
    Raises:
        KeyboardInterrupt: ç”¨æˆ·ä¸­æ–­æ“ä½œæ—¶è¿›è¡Œæ¸…ç†ã€‚
        Exception: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿçš„å…¶ä»–é”™è¯¯ã€‚
    """
    script_path = Path(__file__).parent
    dl_folder = config_manager.get_download_folder(script_path)
    setup_logging(dl_folder)

    log.info("ğŸš€ æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸AIå­—å¹•å·¥å…· v4.0 (Async) å¯åŠ¨ ğŸš€")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸å¤„ç†å·¥å…·")
    parser.add_argument('inputs', nargs='+', help='URLæˆ–æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-b', '--batch-file', action='store_true', help='æ‰¹é‡å¤„ç†æ–‡ä»¶ä¸­çš„URL')
    parser.add_argument('-m', '--mode', choices=['video', 'both', 'audio', 'subtitle'], default='video')
    parser.add_argument('-p', '--proxy', type=str, default=None)
    parser.add_argument('--ai-subs', action='store_true', help='è‡ªåŠ¨ç”ŸæˆAIå­—å¹•')
    args = parser.parse_args()

    # è·å–è¾“å…¥
    inputs = get_inputs(args)
    if not inputs:
        log.error('æ²¡æœ‰æœ‰æ•ˆçš„è¾“å…¥å¯ä¾›å¤„ç†ã€‚')
        return

    # è·å–cookies
    cookies = get_cookies(inputs)
    
    # åˆå§‹åŒ–ä¸‹è½½å™¨å’Œå­—å¹•å¤„ç†å™¨
    downloader = Downloader(dl_folder, cookies, args.proxy)
    sub_processor = SubtitleProcessor(dl_folder, args.proxy) if (args.ai_subs or args.mode == 'subtitle') else None

    # æ ¹æ®æ¨¡å¼å¤„ç†ä»»åŠ¡
    if args.mode == 'subtitle':
        await process_subtitle_tasks(sub_processor, inputs)
    else:
        await process_download_tasks(downloader, sub_processor, inputs, args)

    try:
        # å¤„ç†æ¨¡å¼çš„é”™è¯¯å¤„ç†å·²ç»åœ¨å„ä¸ªé˜¶æ®µå†…éƒ¨å¤„ç†
        pass
    except KeyboardInterrupt:
        log.warning('ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œæ­£åœ¨æ¸…ç†...')
        try:
            await downloader.cleanup_all_incomplete_files()
        except Exception as cleanup_error:
            log.error(f'æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {cleanup_error}')
        console.print('âœ… æ¸…ç†å®Œæˆï¼Œå®‰å…¨é€€å‡º', style='bold green')
    except Exception as e:
        log.critical(f"å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
    finally:
        log.info("ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆ!")
        log.info(f"ğŸ“ æ—¥å¿—ä¸æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {dl_folder.resolve()}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print('\n\nğŸš« æ“ä½œè¢«ç”¨æˆ·å¼ºåˆ¶å–æ¶ˆã€‚', style='bold red')
    except Exception as e:
        console.print(f'\n\nâŒ ç¨‹åºæ‰§è¡Œæ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}', style='bold red')
        logging.getLogger(__name__).critical(f'æœªå¤„ç†çš„å¼‚å¸¸: {e}', exc_info=True)