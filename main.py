# main.py
import argparse, time, json, re, logging
from datetime import datetime
from pathlib import Path
from typing import Optional

# ä»rich.loggingå¯¼å…¥RichHandlerä»¥ç¾åŒ–æ§åˆ¶å°æ—¥å¿—
from rich.logging import RichHandler

from downloader import Downloader
from subtitles import SubtitleProcessor, AI_LIBRARIES_AVAILABLE

def setup_logging(log_folder: Path):
    """é…ç½®å…¨å±€æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ã€‚"""
    log_file_path = log_folder / "downloader.log"
    
    # é…ç½®åŸºç¡€logger
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - [%(levelname)s] - %(name)s - %(message)s",
        handlers=[
            # æ–‡ä»¶å¤„ç†å™¨ï¼Œè®°å½•æ‰€æœ‰ä¿¡æ¯
            logging.FileHandler(log_file_path, encoding='utf-8'),
            # æ§åˆ¶å°å¤„ç†å™¨ï¼Œä½¿ç”¨richç¾åŒ–
            RichHandler(rich_tracebacks=True, show_path=False)
        ]
    )

def get_urls(args) -> list[str]:
    # ... (æ­¤å‡½æ•°æ— éœ€æ”¹åŠ¨) ...
    urls = []
    if args.batch_file:
        log.info("æ¨¡å¼: ä»æ–‡ä»¶æ‰¹é‡è¯»å–URL")
        for fpath in args.inputs:
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    lines = [l.strip() for l in f if l.strip() and not l.strip().startswith('#')]
                    urls.extend(lines); log.info(f"  - å·²ä» '{fpath}' åŠ è½½ {len(lines)} ä¸ªURLã€‚")
            except FileNotFoundError: log.error(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{fpath}'ï¼Œå·²è·³è¿‡ã€‚")
    else: log.info("æ¨¡å¼: ç›´æ¥ä»å‘½ä»¤è¡Œè¯»å–URL"); urls = args.inputs
    return urls

def sanitize(name: str, max_len: int = 50) -> str:
    # ... (æ­¤å‡½æ•°æ— éœ€æ”¹åŠ¨) ...
    name = re.sub(r'[\\/*?:"<>|]', '_', name).strip()
    return f"{name[:max_len]}..." if len(name) > max_len else name

def save_info(folder: Path, prefix: str):
    json_path = folder / f"{prefix}.info.json"
    if not json_path.exists(): return
    try:
        with open(json_path, 'r', encoding='utf-8') as f: info = json.load(f)
        txt_path = folder / f"{prefix}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            # åŸºæœ¬ä¿¡æ¯
            f.write(f"è§†é¢‘æ ‡é¢˜: {info.get('title', 'N/A')}\n")
            f.write(f"è§†é¢‘ID: {info.get('id', 'N/A')}\n")
            f.write(f"UPä¸»: {info.get('uploader', 'N/A')}\n")
            f.write(f"åŸå§‹URL: {info.get('webpage_url', 'N/A')}\n\n")
            
            # æŠ€æœ¯å‚æ•°
            f.write("=== æŠ€æœ¯å‚æ•° ===\n")
            if 'formats' in info and info['formats']:
                # æ‰¾åˆ°æœ€ä½³è§†é¢‘æ ¼å¼
                video_format = None
                audio_format = None
                
                for fmt in info['formats']:
                    # å¯»æ‰¾æœ€ä½³è§†é¢‘æ ¼å¼
                    if fmt.get('vcodec') != 'none' and fmt.get('height'):
                        if not video_format or (fmt.get('height', 0) > video_format.get('height', 0)):
                            video_format = fmt
                    
                    # å¯»æ‰¾æœ€ä½³éŸ³é¢‘æ ¼å¼
                    if fmt.get('acodec') != 'none' and fmt.get('vcodec') == 'none':
                        if not audio_format or (fmt.get('abr', 0) > audio_format.get('abr', 0)):
                            audio_format = fmt
                
                if video_format:
                    width = video_format.get('width', 'N/A')
                    height = video_format.get('height', 'N/A')
                    f.write(f"åˆ†è¾¨ç‡: {width}x{height}\n")
                    f.write(f"è§†é¢‘ç¼–ç : {video_format.get('vcodec', 'N/A')}\n")
                    
                    # éŸ³é¢‘ç¼–ç ï¼šä¼˜å…ˆä»éŸ³é¢‘æ ¼å¼è·å–ï¼Œå¦åˆ™ä»è§†é¢‘æ ¼å¼è·å–
                    audio_codec = 'N/A'
                    if audio_format and audio_format.get('acodec') != 'none':
                        audio_codec = audio_format.get('acodec')
                    elif video_format.get('acodec') != 'none':
                        audio_codec = video_format.get('acodec')
                    f.write(f"éŸ³é¢‘ç¼–ç : {audio_codec}\n")
                    
                    # æ–‡ä»¶å¤§å°
                    filesize = video_format.get('filesize') or info.get('filesize')
                    if filesize:
                        size_mb = filesize / (1024 * 1024)
                        f.write(f"æ–‡ä»¶å¤§å°: {size_mb:.2f} MB\n")
            f.write("\n")
            
            # ç®€ä»‹å†…å®¹
            description = info.get('description', '')
            if description:
                f.write("------ ç®€ä»‹ ------\n")
                f.write(f"{description}\n")
            
        log.info(f"    ğŸ“„ å·²ç”Ÿæˆä¿¡æ¯æ–‡ä»¶: {txt_path.name}")
    except Exception as e: log.error(f"    ç”Ÿæˆ .txt ä¿¡æ¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    finally: json_path.unlink(missing_ok=True)

def process_item(dlr: Downloader, sub_proc: Optional[SubtitleProcessor], url: str, prefix: str, args: argparse.Namespace):
    # ... (æ­¤å‡½æ•°æ— éœ€æ”¹åŠ¨) ...
    log.info(f"â–¶ï¸ (é¡¹ç›®) å¼€å§‹å¤„ç†: {prefix}")
    log.info(f"ğŸ”— URL: {url}")
    vid_path, aud_path, ai_src = None, None, None
    if dlr.download_metadata(url, prefix): save_info(dlr.download_folder, prefix)
    if args.mode in ['video', 'both']:
        vid_path = dlr.download_and_merge(url, prefix)
        if not vid_path: log.error(f"    è§†é¢‘ä¸‹è½½æˆ–åˆå¹¶å¤±è´¥ï¼Œä¸­æ­¢ã€‚"); dlr.cleanup_temp_files(prefix); return
        ai_src = vid_path
    if args.mode == 'both' and vid_path:
        log.info("-" * 25)
        aud_path = dlr.extract_audio_from_local_file(vid_path, prefix)
        if aud_path: ai_src = aud_path
    if args.ai_subs:
        if ai_src: sub_proc.process_item(prefix, ai_src)
        else: log.warning(f"    æ‰¾ä¸åˆ°ç”¨äºAIè½¬å½•çš„åª’ä½“æ–‡ä»¶ã€‚")
    dlr.cleanup_temp_files(prefix)

def main():
    # åœ¨æ‰€æœ‰æ“ä½œå¼€å§‹å‰ï¼Œå…ˆåˆ›å»ºæ–‡ä»¶å¤¹å¹¶é…ç½®æ—¥å¿—
    dl_folder = Path(datetime.now().strftime("%Y%m%d-%H%M%S"))
    dl_folder.mkdir(exist_ok=True)
    setup_logging(dl_folder)
    
    global log
    log = logging.getLogger(__name__)

    log.info("ğŸš€ æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸AIå­—å¹•å·¥å…· v3.0 (Logging) å¯åŠ¨ ğŸš€")
    
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸å¤„ç†å·¥å…·", formatter_class=argparse.RawTextHelpFormatter)
    # ... (å‚æ•°è§£æé€»è¾‘ä¸å˜) ...
    parser.add_argument("inputs", nargs='+', help="ä¸€ä¸ªæˆ–å¤šä¸ªURLï¼›æˆ–å½“ä½¿ç”¨-bæ—¶ï¼Œä¸ºæ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("-b", "--batch-file", action="store_true", help="å°†è¾“å…¥è§†ä¸ºåŒ…å«URLåˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶ã€‚")
    parser.add_argument("-m", "--mode", choices=['video', 'both'], default='video', help="ä¸‹è½½æ¨¡å¼:\n  video: ä»…ä¸‹è½½è§†é¢‘(é»˜è®¤)\n  both:  ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘")
    parser.add_argument("-p", "--proxy", type=str, default=None, help="è®¾ç½®HTTP/SOCKSä»£ç†ã€‚")
    parser.add_argument("--ai-subs", action="store_true", help="å½“æ— å®˜æ–¹å­—å¹•æ—¶ï¼Œè‡ªåŠ¨ç”ŸæˆAIå­—å¹•ã€‚")
    args = parser.parse_args()

    if args.ai_subs and not AI_LIBRARIES_AVAILABLE:
        log.error("--ai-subs åŠŸèƒ½éœ€è¦ `deep-translator` å’Œ `openai-whisper` åº“ã€‚")
        log.error("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return
        
    urls = get_urls(args)
    if not urls: log.error("æ²¡æœ‰æœ‰æ•ˆçš„URLå¯ä¾›å¤„ç†ã€‚"); return
    
    cookies = str(Path("cookies.txt").resolve()) if Path("cookies.txt").exists() else None
    downloader = Downloader(dl_folder, cookies, args.proxy)
    sub_processor = SubtitleProcessor(dl_folder, args.proxy) if args.ai_subs else None

    log.info(f"æ‰€æœ‰å†…å®¹å°†ä¿å­˜åˆ°: {dl_folder.resolve()}")
    log.info(f"ä¸‹è½½æ¨¡å¼: {args.mode}")
    if args.proxy: log.info(f"ä»£ç†è®¾ç½®: {args.proxy}")
    if args.ai_subs: log.info("AIå­—å¹•ç”Ÿæˆ: å·²å¯ç”¨")
    if cookies: log.info("å·²åŠ è½½Cookiesæ–‡ä»¶")

    total_items = 0
    try:
        for url in urls:
            log.info("============================================================")
            log.info(f"æ­£åœ¨å¤„ç†URL: {url}")
            stream = downloader.stream_playlist_info(url)
            count, has_started = 0, False
            for i, meta in enumerate(stream, 1):
                has_started, count = True, i
                prefix = f"{i:03d}_{sanitize(meta.get('title', f'é¡¹ç›®_{i}'))}"
                process_item(downloader, sub_processor, meta.get('url', url), prefix, args)
                log.info("ç¤¼è²Œç­‰å¾…3ç§’..."); time.sleep(3)
            if not has_started:
                log.warning("æœªèƒ½ä»æµä¸­è§£æåˆ°é¡¹ç›®ï¼Œå°è¯•ä½œä¸ºå•ä¸ªé“¾æ¥å¤„ç†...")
                prefix = f"001_{sanitize(urls[0] if len(urls)==1 else 'å•é¡¹ä¸‹è½½')}"
                process_item(downloader, sub_processor, url, prefix, args); count = 1
            total_items += count
            log.info(f"--- URLå¤„ç†å®Œæˆ: {url} | å…±å¤„ç†äº† {count} ä¸ªé¡¹ç›® ---")
    except KeyboardInterrupt: log.warning("\n\nç”¨æˆ·ä¸­æ–­äº†æ“ä½œã€‚")
    except Exception as e: log.critical(f"å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True) # exc_info=Trueä¼šè®°å½•å®Œæ•´çš„é”™è¯¯å †æ ˆ
    finally:
        log.info("============================================================")
        log.info(f"ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆ! æœ¬æ¬¡è¿è¡Œå…±å¤„ç† {total_items} ä¸ªé¡¹ç›®ã€‚")
        log.info(f"ğŸ“ æ—¥å¿—ä¸æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {dl_folder.resolve()}")

if __name__ == "__main__":
    main()