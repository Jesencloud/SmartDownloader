# main.py
import argparse, time, json, re
from datetime import datetime
from pathlib import Path
from typing import Optional, List

# ã€æ–°ã€‘å¢åŠ å¹¶å‘åº“å¯¼å…¥
from concurrent.futures import ThreadPoolExecutor

from downloader import Downloader
from subtitles import SubtitleProcessor, AI_LIBRARIES_AVAILABLE

# ... (get_urls, sanitize, save_info, process_item å‡½æ•°æ— éœ€æ”¹åŠ¨) ...
def get_urls(args) -> list[str]:
    urls = []
    if args.batch_file:
        print("æ¨¡å¼: ä»æ–‡ä»¶æ‰¹é‡è¯»å–URL")
        for fpath in args.inputs:
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    lines = [l.strip() for l in f if l.strip() and not l.strip().startswith('#')]
                    urls.extend(lines); print(f"  - å·²ä» '{fpath}' åŠ è½½ {len(lines)} ä¸ªURLã€‚")
            except FileNotFoundError: print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{fpath}'ï¼Œå·²è·³è¿‡ã€‚")
    else: print("æ¨¡å¼: ç›´æ¥ä»å‘½ä»¤è¡Œè¯»å–URL"); urls = args.inputs
    return urls

def sanitize(name: str, max_len: int = 50) -> str:
    name = re.sub(r'[\\/*?:"<>|]', '_', name).strip()
    return f"{name[:max_len]}..." if len(name) > max_len else name

def save_info(folder: Path, prefix: str):
    json_path = folder / f"{prefix}.info.json"
    if not json_path.exists(): return
    try:
        with open(json_path, 'r', encoding='utf-8') as f: info = json.load(f)
        txt_path = folder / f"{prefix}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {info.get('title', 'N/A')}\nID: {info.get('id', 'N/A')}\n")
            f.write(f"ä½œè€…: {info.get('uploader', 'N/A')}\nURL: {info.get('webpage_url', 'N/A')}\n")
        print(f"    ğŸ“„ å·²ç”Ÿæˆä¿¡æ¯æ–‡ä»¶: {txt_path.name}")
    except Exception as e: print(f"    âŒ ç”Ÿæˆ .txt ä¿¡æ¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    finally: json_path.unlink(missing_ok=True)

def process_item(dlr: Downloader, sub_proc: Optional[SubtitleProcessor], url: str, prefix: str, args: argparse.Namespace):
    print("\n" + "="*50); print(f"â–¶ï¸ (é¡¹ç›®) å¼€å§‹å¤„ç†: {prefix}"); print(f"ğŸ”— URL: {url}")
    vid_path, aud_path, ai_src = None, None, None
    if dlr.download_metadata(url, prefix): 
        save_info(dlr.download_folder, prefix)
    if args.mode in ['video', 'both']:
        temp_vid, temp_aud = dlr.download_parts(url, prefix)
        if temp_vid and temp_aud:
            if temp_vid == temp_aud:
                vid_path = dlr.download_folder / f"{prefix}.mp4"; temp_vid.rename(vid_path)
            else: vid_path = dlr.merge_to_mp4(temp_vid, temp_aud, prefix)
            ai_src = vid_path
        else: print(f"    âŒ è§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œä¸­æ­¢ã€‚"); dlr.cleanup_temp_files(prefix); return
    if args.mode == 'both' and vid_path:
        print("-" * 25); aud_path = dlr.extract_audio_from_local_file(vid_path, prefix)
        if aud_path: ai_src = aud_path
    if args.ai_subs:
        if ai_src: sub_proc.process_item(prefix, ai_src)
        else: print(f"    âš ï¸ æ‰¾ä¸åˆ°ç”¨äºAIè½¬å½•çš„åª’ä½“æ–‡ä»¶ã€‚")
    dlr.cleanup_temp_files(prefix)


def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åª’ä½“ä¸‹è½½ä¸å¤„ç†å·¥å…·", formatter_class=argparse.RawTextHelpFormatter)
    
    # ã€æ–°ã€‘ä½¿ç”¨äº’æ–¥ç»„æ¥ç¡®ä¿URLè¾“å…¥å’Œæ–‡ä»¶ç¿»è¯‘åŠŸèƒ½ä¸å†²çª
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("inputs", nargs='*', default=None, help="ä¸€ä¸ªæˆ–å¤šä¸ªURLï¼›æˆ–å½“ä½¿ç”¨-bæ—¶ï¼Œä¸ºæ–‡ä»¶è·¯å¾„ã€‚")
    group.add_argument("--translate-file", type=str, help="ä»…å¯¹æŒ‡å®šçš„æœ¬åœ°SRTæ–‡ä»¶æ‰§è¡Œç¿»è¯‘å’Œåˆå¹¶æ“ä½œã€‚")

    parser.add_argument("-b", "--batch-file", action="store_true", help="[ä¸‹è½½æ¨¡å¼] å°†è¾“å…¥è§†ä¸ºåŒ…å«URLåˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶ã€‚")
    parser.add_argument("-m", "--mode", choices=['video', 'both'], default='video', help="[ä¸‹è½½æ¨¡å¼] ä¸‹è½½æ¨¡å¼:\n  video: ä»…ä¸‹è½½è§†é¢‘(é»˜è®¤)\n  both:  ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘")
    parser.add_argument("-p", "--proxy", type=str, default=None, help="è®¾ç½®HTTP/SOCKSä»£ç†ã€‚")
    parser.add_argument("--ai-subs", action="store_true", help="[ä¸‹è½½æ¨¡å¼] å½“æ— å®˜æ–¹å­—å¹•æ—¶ï¼Œè‡ªåŠ¨ç”ŸæˆAIå­—å¹•ã€‚")
    args = parser.parse_args()

    # --- æ–°çš„ç¿»è¯‘æ¨¡å¼é€»è¾‘ ---
    if args.translate_file:
        print("æ¨¡å¼: ç‹¬ç«‹ç¿»è¯‘æ¨¡å¼")
        srt_path = Path(args.translate_file)
        if not srt_path.exists() or srt_path.suffix.lower() != '.srt':
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ª .srt æ–‡ä»¶ -> {srt_path}")
            return
        
        # åˆå§‹åŒ–ä¸€ä¸ªä¸´æ—¶çš„ä¸‹è½½æ–‡ä»¶å¤¹å’Œå­—å¹•å¤„ç†å™¨
        dl_folder = srt_path.parent
        sub_processor = SubtitleProcessor(dl_folder, args.proxy)
        print(f"å¤„ç†æ–‡ä»¶: {srt_path.name}")
        
        zh_srt = sub_processor.translate(srt_path)
        if zh_srt:
            sub_processor.merge(srt_path, zh_srt)
        
        print("\nğŸ‰ ç¿»è¯‘ä»»åŠ¡å®Œæˆ!")
        return # ç¿»è¯‘å®Œæˆï¼Œç›´æ¥é€€å‡ºç¨‹åº

    # --- åŸæœ‰çš„ä¸‹è½½æ¨¡å¼é€»è¾‘ ---
    if args.ai_subs and not AI_LIBRARIES_AVAILABLE:
        print("âŒ é”™è¯¯: --ai-subs åŠŸèƒ½éœ€è¦ `deep-translator` å’Œ `openai-whisper` åº“ã€‚"); return
        
    urls = get_urls(args)
    if not urls: print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„URLå¯ä¾›å¤„ç†ã€‚"); return
    
    dl_folder = Path(datetime.now().strftime("%Y%m%d-%H%M%S"))
    dl_folder.mkdir(exist_ok=True)
    cookies = str(Path("cookies.txt").resolve()) if Path("cookies.txt").exists() else None
    downloader = Downloader(dl_folder, cookies, args.proxy)
    sub_processor = SubtitleProcessor(dl_folder, args.proxy) if args.ai_subs else None

    print(f"ğŸ“‚ æ‰€æœ‰å†…å®¹å°†ä¿å­˜åˆ°: {dl_folder.resolve()}"); print(f"ä¸‹è½½æ¨¡å¼: {args.mode}")
    if args.proxy: print(f"ä»£ç†è®¾ç½®: {args.proxy}")
    if args.ai_subs: print("AIå­—å¹•ç”Ÿæˆ: å·²å¯ç”¨")
    if cookies: print(f"âœ… å·²åŠ è½½Cookiesæ–‡ä»¶")

    total_items = 0
    try:
        for url in urls:
            print("\n" + "#"*60); print(f"æ­£åœ¨å¤„ç†URL: {url}"); print("#"*60)
            stream = downloader.stream_playlist_info(url)
            count, has_started = 0, False
            for i, meta in enumerate(stream, 1):
                has_started, count = True, i
                prefix = f"{i:03d}_{sanitize(meta.get('title', f'é¡¹ç›®_{i}'))}"
                process_item(downloader, sub_processor, meta.get('url', url), prefix, args)
                print("\nâ³ ç¤¼è²Œç­‰å¾…3ç§’..."); time.sleep(3)
            if not has_started:
                print("\nğŸ¤” æœªèƒ½ä»æµä¸­è§£æåˆ°é¡¹ç›®ï¼Œå°è¯•ä½œä¸ºå•ä¸ªé“¾æ¥å¤„ç†...")
                prefix = f"001_{sanitize(urls[0] if len(urls)==1 else 'å•é¡¹ä¸‹è½½')}"
                process_item(downloader, sub_processor, url, prefix, args); count = 1
            total_items += count
            print(f"\n--- URLå¤„ç†å®Œæˆ: {url} | å…±å¤„ç†äº† {count} ä¸ªé¡¹ç›® ---")
    except KeyboardInterrupt: print("\n\nç”¨æˆ·ä¸­æ–­äº†æ“ä½œã€‚")
    except Exception as e: print(f"\nâš ï¸ å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
    finally:
        print("\n" + "ğŸ‰"*20); print(f"å…¨éƒ¨ä»»åŠ¡å®Œæˆ! æœ¬æ¬¡è¿è¡Œå…±å¤„ç† {total_items} ä¸ªé¡¹ç›®ã€‚"); print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {dl_folder.resolve()}")


if __name__ == "__main__":
    main()